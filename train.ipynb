{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2185e1a0-a266-4cda-9d47-0805a985e512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import requests\n",
    "from model import *\n",
    "from my_tokenizer.regex import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84a7f541-11f3-4827-a74b-e4bd29585c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete: data/input.txt\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import requests\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(\"data/input.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response.text)\n",
    "\n",
    "print(\"Download complete: data/input.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48954213-802c-4413-a79c-7cd51fbbc3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPEDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, seq_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        print(\"Encoding text...\")\n",
    "        # Encode the entire text into integers using the trained tokenizer\n",
    "        self.data = torch.tensor(self.tokenizer.encode(text), dtype=torch.long)\n",
    "        print(f\"Total tokens: {len(self.data)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = torch.randint(0, len(self.data) - self.seq_len - 1, (1,)).item()\n",
    "        chunk = self.data[start : start + self.seq_len + 1]\n",
    "        x = chunk[:-1]\n",
    "        y = chunk[1:]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97350fba-6e9d-4538-aa27-4047b0a27233",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/input.txt', 'r', encoding='utf-8') as f:\n",
    "        text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a370a98-6c84-48a4-b2de-7e44ca6007f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e634a54-63a4-42c9-8b66-605fcbe959ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Tokenizer (this may take a moment)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Tokenizer...: 100%|█| 256/256 [01:56<00:00,  2.20it/s, last_merge=(262,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer trained!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Tokenizer\n",
    "print(\"Training Tokenizer (this may take a moment)...\")\n",
    "tokenizer = RegexTokenizer()\n",
    "# vocab_size=512 for speed. GPT-4 uses ~100k.\n",
    "tokenizer.train(text, vocab_size=512, verbose=False)\n",
    "print(\"Tokenizer trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a9784ad-5a12-4185-8878-0fd39b3b049c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text...\n",
      "Total tokens: 658028\n"
     ]
    }
   ],
   "source": [
    "dataset = BPEDataset(text, tokenizer, seq_len=128)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "430f12b9-67d7-4ab4-9c6e-73c3f367cc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([32, 128])\n",
      "y: torch.Size([32, 128])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(f\"x: {batch[0].shape}\\ny: {batch[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5db2001-e07e-4984-a3d4-15c0c7c3f8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[262, 296, 114,  ...,  58, 291, 109],\n",
       "        [256, 271, 110,  ..., 318, 333, 110],\n",
       "        [385, 274,  44,  ..., 121, 269, 114],\n",
       "        ...,\n",
       "        [ 77, 121, 306,  ...,  99, 121,  59],\n",
       "        [384,  10,  89,  ..., 102, 276, 333],\n",
       "        [267, 365, 332,  ..., 263, 114, 276]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eadf7583-aded-4590-a6ee-f02b4d6e2844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cpu\n",
      "Model built successfully.\n"
     ]
    }
   ],
   "source": [
    "config = Config(vocab_size=512, embed_size=256, seq_len=128, n_layer=2, h=2, d_ff=128, total_epochs=1, lr=1e-3, dropout=0.0)    \n",
    "print(f\"Running on: {config.device}\")\n",
    "\n",
    "model = GPT.build_gpt(config)\n",
    "print(\"Model built successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55628893-b829-4919-ad41-1ba4e806b3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Token ID in Data: 511\n",
      "Model Vocab Size: 512\n",
      "\n",
      "Status: Vocabulary size looks consistent.\n"
     ]
    }
   ],
   "source": [
    "# 1. Get a single batch from your data loader\n",
    "data_iter = iter(train_loader)\n",
    "x, y = next(data_iter)\n",
    "\n",
    "# 2. Now run the check\n",
    "print(f\"Max Token ID in Data: {x.max().item()}\")\n",
    "print(f\"Model Vocab Size: {model.config.vocab_size}\")\n",
    "\n",
    "if x.max().item() >= model.config.vocab_size:\n",
    "    print(\"\\nCRITICAL ERROR FOUND:\")\n",
    "    print(f\"Your data contains token ID {x.max().item()}, but your model only knows up to {model.config.vocab_size - 1}.\")\n",
    "    print(\"FIX: You need to re-initialize the model with the correct vocab_size.\")\n",
    "else:\n",
    "    print(\"\\nStatus: Vocabulary size looks consistent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f32bc5bc-7bce-48f7-a1fe-732eed241bb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0/1: 100%|███████████████████| 20560/20560 [1:59:59<00:00,  2.86it/s, loss=1.97]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/1 Loss: 2.1431859313745907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = GPT.build_gpt(config)\n",
    "print(f\"Training on {config.device}...\")\n",
    "model.train_gpt(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12ecacb4-53c9-43c2-a160-d9cb58380b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating text\n",
      "\n",
      "input: The\n",
      "\n",
      "output: \n",
      "\n",
      "The King Edward's judge,\n",
      "And he shall be my souls of my fellow,\n",
      "Whose friends, and Warwick, he is an enemy\n",
      "Shall be true that I did a pupil.\n",
      "Ah, cousin York, or I say it, that\n",
      "My presumble at that we may durst.\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "I am encounterfeit is.\n",
      "\n",
      "GRUMIO:\n",
      "Alack, madam, and my gage.\n",
      "\n",
      "MARIANA:\n",
      "It is a\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nGenerating text\\n\")\n",
    "model.eval()\n",
    "start_tokens = torch.tensor([tokenizer.encode(\"The\")], device=config.device)\n",
    "generated = model.generate(start_tokens, max_new_token=200, top_k=5)\n",
    "\n",
    "# Decode back to text\n",
    "decoded = tokenizer.decode(generated[0].tolist())\n",
    "print(f\"input: The\\n\")\n",
    "print(f\"output: \\n\\n{decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81779aa3-e202-43b6-b10b-5d0f397f3ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
